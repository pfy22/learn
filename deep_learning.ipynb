{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始学习并测试cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu118'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "torch.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本常识测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 生成一个6*4的0矩阵\n",
    "A = torch.zeros(6, 4)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5235, 0.9180, 0.8760, 0.8235],\n",
      "        [0.7577, 0.5396, 0.1096, 0.9080],\n",
      "        [0.3327, 0.1300, 0.3039, 0.0740],\n",
      "        [0.0786, 0.1658, 0.9320, 0.2650],\n",
      "        [0.9540, 0.4197, 0.3018, 0.2270],\n",
      "        [0.4086, 0.2787, 0.3308, 0.0029]])\n"
     ]
    }
   ],
   "source": [
    "# 生成一个6*4均匀随机矩阵\n",
    "B = torch.rand(6, 4)\n",
    "print(B)       # 注意随机矩阵每次刷新生成结果都会不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0710, -0.2848, -1.0360,  0.1408],\n",
      "         [-0.7641,  0.1686,  0.0895,  1.3284],\n",
      "         [-1.4051,  0.9343,  0.4535, -1.4252],\n",
      "         [ 1.4966, -0.7298, -0.1376, -0.2596],\n",
      "         [-0.2085, -0.2545,  1.0301, -0.9938],\n",
      "         [ 1.3350,  0.7056, -1.2472, -0.8449]],\n",
      "\n",
      "        [[-0.6459,  0.9148,  0.7896,  0.2088],\n",
      "         [-0.1468, -0.6042, -0.6629,  0.3512],\n",
      "         [ 0.9237, -0.7059,  0.9575,  1.3783],\n",
      "         [-0.6945, -0.1184,  1.0462, -0.9712],\n",
      "         [ 0.7495,  2.1200, -0.3671, -1.1055],\n",
      "         [ 0.0883, -1.1902, -0.7211, -2.2528]],\n",
      "\n",
      "        [[-0.5258, -0.3549, -0.4089, -2.5850],\n",
      "         [-0.6442, -0.8657, -0.7392, -0.7339],\n",
      "         [-1.5654,  1.0170, -0.6202, -1.2822],\n",
      "         [-0.8146, -1.0296,  0.4203,  1.7127],\n",
      "         [-1.5226,  2.4463,  1.0782, -0.3357],\n",
      "         [-1.2463,  0.2228,  1.2998,  1.6104]]])\n"
     ]
    }
   ],
   "source": [
    "# 生成一个3*6*4高斯随机数组\n",
    "C = torch.randn(3, 6, 4)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.9082e-39, 8.9082e-39, 8.9082e-39],\n",
       "        [1.0194e-38, 9.1837e-39, 4.6837e-39],\n",
       "        [9.2755e-39, 1.0837e-38, 8.4490e-39],\n",
       "        [9.9184e-39, 9.9184e-39, 9.0000e-39],\n",
       "        [1.0561e-38, 1.0653e-38, 4.1327e-39]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个5*3的空矩阵\n",
    "torch.empty(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor()可以转化成array()\n",
    "import torch\n",
    "x_1 = torch.ones(5, 3)\n",
    "x_2 = x_1.numpy()     # 注意这里的numpy()是torch里面的函数，不需要调用numpy库\n",
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# array()也可以转换成tensor()\n",
    "import torch\n",
    "x_3 = torch.from_numpy(x_2)\n",
    "print(x_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 16, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建一个输入大小为(3, 64, 64)的张量\n",
    "input = torch.randn(100, 3, 64, 64)\n",
    "\n",
    "# 创建一个二维卷积层\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# 将输入传递给卷积层\n",
    "output = conv_layer(input)\n",
    "\n",
    "# 输出结果的大小\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 复现线性回归\n",
    "- 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [10.]]\n",
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
      "[[ 1.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [13.]\n",
      " [15.]\n",
      " [17.]\n",
      " [19.]\n",
      " [21.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_values = [i for i in range(11)]     # 注意python可以使用这种方式写一个list\n",
    "print(x_values)\n",
    "x_train = np.array(x_values, dtype=np.float32)     # 注意dtype=np.float32不可省略\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "print(x_train)\n",
    "x_train.shape\n",
    "\n",
    "y_values = [2*i+1 for i in x_values]     # 注意python可以使用这种方式写一个list\n",
    "print(y_values)\n",
    "y_train = np.array(y_values, dtype=np.float32)     # 注意dtype=np.float32不可省略\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "print(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = gpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LinearRegressionModel(nn.Module):        # 继承父类nn.Module\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)    # 定义一个名为linear的类内对象\n",
    "    \n",
    "    def forward(self, x):                      # 复写父类函数——前向传播函数foward(self, ....)\n",
    "        return self.linear(x)                  # 由于父类当中包含魔术方法__call__(self, ...),故可以在对象后面传入参数x\n",
    "\n",
    "model = LinearRegressionModel(1, 1)            # 输入和输出维度均为1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"device = gpu\")\n",
    "else:\n",
    "    print(\"device = cpu\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练轮数\n",
    "epochs = 1001\n",
    "\n",
    "# 定义学习率(即每一轮优化的步长)\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 定义一个对象optimizer，这是一个优化器，SGD表示随机梯度下降优化器，model.parameters()表示对model里面的全部参数进行优化\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)    \n",
    "\n",
    "# 定义一个均方差损失函数\n",
    "creterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:00<00:00, 1730.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1000,loss=1.565551854820768e-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用tqdm库的tqdm模块去展示训练进程\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    # 每一轮都让轮次+1\n",
    "    epoch += 1\n",
    "\n",
    "    # 更改输入数据类型\n",
    "    inputs = torch.from_numpy(x_train).to(device)\n",
    "    labels = torch.from_numpy(y_train).to(device)     # 将样本当中实际（绝对正确的）的y作为样本的labels\n",
    "    \n",
    "    # 设置可导\n",
    "    inputs.requires_grad_()\n",
    "    labels.requires_grad_()\n",
    "\n",
    "    # 梯度要清零每一次迭代\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 前向传播\n",
    "    outputs = model.forward(inputs)\n",
    "\n",
    "    # 计算每一轮训练的损失,对比模型预测值outputs与正确的labels的之间的差异\n",
    "    loss = creterion(outputs, labels)    \n",
    "\n",
    "    # 反向传播,从数学上来说是对loss()的全部参数求导\n",
    "    loss.backward()\n",
    "\n",
    "    # 更新权重参数\n",
    "    optimizer.step()\n",
    "\n",
    "    # 每隔100轮训练输出一次\n",
    "    if epoch == 1000:\n",
    "        print(f\"epoch={epoch},loss={loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 测试模型预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正确结果[[ 1.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [13.]\n",
      " [15.]\n",
      " [17.]\n",
      " [19.]\n",
      " [21.]]\n",
      "预测结果tensor([[ 1.0000],\n",
      "        [ 3.0000],\n",
      "        [ 5.0000],\n",
      "        [ 7.0000],\n",
      "        [ 9.0000],\n",
      "        [11.0000],\n",
      "        [13.0000],\n",
      "        [15.0000],\n",
      "        [17.0000],\n",
      "        [19.0000],\n",
      "        [21.0000]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "predicted = model(torch.from_numpy(x_train).requires_grad_().to(device))\n",
    "print(f\"正确结果{y_train}\")\n",
    "print(f\"预测结果{predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n这段代码使用PyTorch中的`torch.save()`函数将模型的参数保存到文件中，文件名为'model.pkl'。\\n\\n具体解释如下：\\n1 `torch.save()`: 这是PyTorch库中的一个函数,用于将Python对象保存到磁盘。它可以用来保存模型的参数、张量、字典等。\\n在这里，我们使用`torch.save()`函数来保存模型的参数。\\n\\n2 `model.state_dict()`: 这是一个用于返回模型参数的方法。在PyTorch中,模型的参数通常存储在一个名为`state_dict`的字典中。\\n`model.state_dict()`用于获取模型的所有参数，并以字典的形式返回。\\n\\n3 `'linear.pkl'`: 这是保存模型参数的文件名。在这个例子中，模型的参数将会保存到名为'model.pkl'的文件中。\\n综合起来，`torch.save(model.state_dict(), 'model.pkl')`这行代码的作用是将模型的参数保存到一个名为'model.pkl'的文件中，\\n以便在以后可以重新加载模型的参数，或者在其他地方使用这些参数进行推断或训练。通常，保存模型参数是为了在需要时可以方便地加载模型的状态，\\n避免重新训练模型，节省时间和资源。\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"linear.pkl\")     # 默认保存到当前文件夹下，也可以更改路径\n",
    "\n",
    "\"\"\"\n",
    "这段代码使用PyTorch中的`torch.save()`函数将模型的参数保存到文件中，文件名为'model.pkl'。\n",
    "\n",
    "具体解释如下：\n",
    "1 `torch.save()`: 这是PyTorch库中的一个函数,用于将Python对象保存到磁盘。它可以用来保存模型的参数、张量、字典等。\n",
    "在这里，我们使用`torch.save()`函数来保存模型的参数。\n",
    "\n",
    "2 `model.state_dict()`: 这是一个用于返回模型参数的方法。在PyTorch中,模型的参数通常存储在一个名为`state_dict`的字典中。\n",
    "`model.state_dict()`用于获取模型的所有参数，并以字典的形式返回。\n",
    "\n",
    "3 `'linear.pkl'`: 这是保存模型参数的文件名。在这个例子中，模型的参数将会保存到名为'model.pkl'的文件中。\n",
    "综合起来，`torch.save(model.state_dict(), 'model.pkl')`这行代码的作用是将模型的参数保存到一个名为'model.pkl'的文件中，\n",
    "以便在以后可以重新加载模型的参数，或者在其他地方使用这些参数进行推断或训练。通常，保存模型参数是为了在需要时可以方便地加载模型的状态，\n",
    "避免重新训练模型，节省时间和资源。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 读取保存的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n这段代码使用PyTorch中的`torch.load()`函数加载之前保存在文件中的模型参数，并使用`model.load_state_dict()`方法将加载的参数设置到模型中。\\n\\n具体解释如下：\\n1 `torch.load('model.pkl')`: 这是PyTorch库中的一个函数，用于从磁盘加载保存的Python对象。\\n在这里，我们使用`torch.load()`函数从名为'model.pkl'的文件中加载模型的参数。这个文件之前是用`torch.save()`函数保存的模型参数。\\n\\n2 `model.load_state_dict()`: 这是一个用于加载模型参数的方法。\\n在PyTorch中，模型的参数通常存储在一个名为`state_dict`的字典中。\\n`model.load_state_dict()`方法用于将加载的参数设置到模型中，使得模型的参数变成加载的参数。\\n\\n3 综合起来，`model.load_state_dict(torch.load('model.pkl'))`这行代码的作用是从名为'model.pkl'的文件中加载之前保存的模型参数，\\n并将这些参数设置到`model`中，恢复模型的状态。这样，我们就可以继续使用这个模型进行推断、预测或训练，\\n而无需重新训练模型。这对于避免重新训练模型，节省时间和资源非常有用。\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"linear.pkl\"))\n",
    "\"\"\"\n",
    "这段代码使用PyTorch中的`torch.load()`函数加载之前保存在文件中的模型参数，并使用`model.load_state_dict()`方法将加载的参数设置到模型中。\n",
    "\n",
    "具体解释如下：\n",
    "1 `torch.load('model.pkl')`: 这是PyTorch库中的一个函数，用于从磁盘加载保存的Python对象。\n",
    "在这里，我们使用`torch.load()`函数从名为'model.pkl'的文件中加载模型的参数。这个文件之前是用`torch.save()`函数保存的模型参数。\n",
    "\n",
    "2 `model.load_state_dict()`: 这是一个用于加载模型参数的方法。\n",
    "在PyTorch中，模型的参数通常存储在一个名为`state_dict`的字典中。\n",
    "`model.load_state_dict()`方法用于将加载的参数设置到模型中，使得模型的参数变成加载的参数。\n",
    "\n",
    "3 综合起来，`model.load_state_dict(torch.load('model.pkl'))`这行代码的作用是从名为'model.pkl'的文件中加载之前保存的模型参数，\n",
    "并将这些参数设置到`model`中，恢复模型的状态。这样，我们就可以继续使用这个模型进行推断、预测或训练，\n",
    "而无需重新训练模型。这对于避免重新训练模型，节省时间和资源非常有用。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用保存好的训练模型预测新的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test = [[21.]\n",
      " [22.]\n",
      " [23.]\n",
      " [24.]\n",
      " [25.]\n",
      " [26.]\n",
      " [27.]\n",
      " [28.]\n",
      " [29.]\n",
      " [30.]\n",
      " [31.]\n",
      " [32.]\n",
      " [33.]\n",
      " [34.]\n",
      " [35.]\n",
      " [36.]\n",
      " [37.]\n",
      " [38.]\n",
      " [39.]\n",
      " [40.]]\n",
      "预测结果为tensor([[43.0000],\n",
      "        [45.0000],\n",
      "        [47.0000],\n",
      "        [49.0000],\n",
      "        [51.0000],\n",
      "        [53.0000],\n",
      "        [55.0000],\n",
      "        [57.0000],\n",
      "        [59.0000],\n",
      "        [61.0000],\n",
      "        [63.0000],\n",
      "        [65.0000],\n",
      "        [67.0000],\n",
      "        [69.0000],\n",
      "        [71.0000],\n",
      "        [73.0000],\n",
      "        [75.0000],\n",
      "        [77.0000],\n",
      "        [79.0000],\n",
      "        [81.0000]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 制造新的数据\n",
    "x_value1 = [i for i in range(21, 41)]\n",
    "x_test1 = np.array(x_value1, dtype=np.float32)\n",
    "x_test1 = x_test1.reshape(-1, 1)\n",
    "print(f\"x_test = {x_test1}\")\n",
    "x_test1.shape\n",
    "\n",
    "# 利用训练好的模型进行预测\n",
    "predicted1 = model(torch.from_numpy(x_test1).requires_grad_().to(device))\n",
    "print(f\"预测结果为{predicted1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
